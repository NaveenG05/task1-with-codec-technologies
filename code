##1. Create and Save Sample Dataset
import pandas as pd
data = {
    'CustomerID': ['C001', 'C002', 'C003', 'C004', 'C005', 'C006', 'C007', 'C008'],
    'Gender': ['Male', 'Female', 'Female', 'Male', 'Female', 'Male', 'Male', 'Female'],
    'SeniorCitizen': [0, 1, 0, 0, 1, 0, 1, 0],
    'Partner': ['Yes', 'No', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes'],
    'Dependents': ['No', 'No', 'Yes', 'No', 'Yes', 'Yes', 'No', 'Yes'],
    'Tenure': [1, 34, 2, 45, 5, 20, 10, 25],
    'PhoneService': ['Yes', 'Yes', 'No', 'Yes', 'Yes', 'Yes', 'No', 'Yes'],
    'InternetService': ['DSL', 'Fiber optic', 'DSL', 'Fiber optic', 'DSL', 'DSL', 'Fiber optic', 'DSL'],
    'MonthlyCharges': [29.85, 56.95, 53.85, 42.30, 70.70, 45.00, 60.00, 65.00],
    'TotalCharges': [29.85, 1889.5, 108.15, 1840.75, 151.65, 900.00, 600.00, 1400.00],
    'Churn': ['No', 'Yes', 'No', 'No', 'Yes', 'No', 'Yes', 'No']
}
df = pd.DataFrame(data)
df.to_csv("churn_data.csv", index=False)
print("The sample datasets are created and saved as 'churn_data.csv'")
2. Load Data and Preprocess
import pandas as pd
from sklearn.preprocessing import LabelEncoder
# Load dataset
df = pd.read_csv("churn_data.csv")
# Drop CustomerID
df.drop('CustomerID', axis=1, inplace=True)
# Encode all object (string) columns
le = LabelEncoder()
for col in df.select_dtypes(include='object').columns:
    if col != 'Churn':
        df[col] = le.fit_transform(df[col])
# Encode target column
df['Churn'] = df['Churn'].map({'Yes': 1, 'No': 0})
print("The data is loaded and preprocessed")
3.Exploratory Data Analysis (EDA)
import seaborn as sns
import matplotlib.pyplot as plt
print(df.info())
print(df.describe())
print(df.isnull().sum())
# Churn distribution
sns.countplot(x='Churn', data=df)
plt.title("Churn Count")
plt.show()
# Correlation heatmap
sns.heatmap(df.corr(), annot=True, cmap="coolwarm")
plt.title("Correlation Heatmap")
plt.show()
4. Split the Data
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler
# Encode categorical variables (example: 'Gender' only)
df['Gender'] = LabelEncoder().fit_transform(df['Gender'])
# Check if 'Churn' column exists and has enough class variety
if 'Churn' in df.columns and df['Churn'].nunique() == 2:
    X = df.drop('Churn', axis=1)
    y = df['Churn'].map({'Yes': 1, 'No': 0})  # Binary encoding
    if len(df) >= 10:
        # Stratified split (preferred if data is enough)
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, stratify=y, random_state=42
        )
    else:
        # Small data fallback (no stratify)
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=42
        )
    print("Data split successful!")
else:
    print("⚠️ Check your 'Churn' column. It might be missing or not binary.")
    print("The sample data is splited")
5. Scale the Features
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)
6. Train Models
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier
models = {
    "Logistic Regression": LogisticRegression(max_iter=1000),
    "Random Forest": RandomForestClassifier(n_estimators=100),
    "XGBoost": XGBClassifier(use_label_encoder=False, eval_metric='logloss')
}
 7. Evaluate Models
from sklearn.preprocessing import LabelEncoder
# Drop CustomerID column safely (case-insensitive match)
for col in df.columns:
    if col.lower() == 'customerid':
        df.drop(col, axis=1, inplace=True)
        break
# Encode all object (string) columns using LabelEncoder
le = LabelEncoder()
cat_cols = df.select_dtypes(include=['object']).columns
for col in cat_cols:
    df[col] = le.fit_transform(df[col].astype(str))  # Convert to string in case of mixed types
